# Load reticulate
library("reticulate")
# Notice there is a new environment, "spacy_condaenv"
conda_list()
# Use the "spacy_condaenv" environment
use_condaenv(condaenv = "spacy_condaenv")
# Load spacyr library
library("spacyr")
# Get english model from the package
spacy_initialize(model = "en_core_web_sm")
pd = import("pandas")
# Read in some text
txt = c(d1 = "noRth is an R conference.",
d2 = "this is the best conference ever!",
d3 = "Minnesota is in the midwest.")
# Check the components of the text
# pos is the Universal tagset for parts-of-speech
# tag is the detailed tag set based on the TIGER Treebank scheme
(wordbreakdown = spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE))
rm(list = ls())
.rs.restartR()
# Interactive R code to accompany presentation #
# Load library
library(reticulate)
# Check the package version
packageVersion("reticulate")
# Check what Python source it's using
py_config()
# Do a quick iteractive session
# This code will look like errors in script, but it's meant to be run in an interactive session
repl_python()
p_df
# Notice it doesn't exist! But if we restart our interactive session
repl_python()
# Load the os (operating system) package
# This package is pre-installed
os = import("os")
os$getcwd()
getwd()
# Import modules from the package
sklearn_text = import("sklearn.feature_extraction.text")
source_python("logitfunc.py")
logit_py(0.5)
logit_py(1)
rm(list = ls())
.rs.restartR()
# Load reticulate
library("reticulate")
# Notice there is a new environment, "spacy_condaenv"
conda_list()
# Use the "spacy_condaenv" environment
use_condaenv(condaenv = "spacy_condaenv")
# Load spacyr library
library("spacyr")
# Get english model from the package
spacy_initialize(model = "en_core_web_sm")
pd = import("pandas")
txt = c(d1 = "noRth is an R conference.",
d2 = "this is the best conference ever!",
d3 = "Minnesota is in the midwest.")
txt
# Check the components of the text
# pos is the Universal tagset for parts-of-speech
# tag is the detailed tag set based on the TIGER Treebank scheme
(wordbreakdown = spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE))
# Tokenize (aka split up) the sentences
spacy_tokenize(txt)$d1
# Now break up the text into phrases
parsedtxt = spacy_parse(txt, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
nounphrase_extract(parsedtxt)
# Let's go back to the word breakdown
wordbreakdown
class(wordbreakdown)
# Convert the parsed text to a pandas dataframe
ex_pandas_df = reticulate::r_to_py(wordbreakdown)
class(ex_pandas_df)
# Create a ggplot histogram of the number word types
wordbreakdown_df = data.frame(wordbreakdown)
library(ggplot2)
ggplot(wordbreakdown_df, aes(pos)) +
geom_bar() +
xlab("Parts of Speech")
